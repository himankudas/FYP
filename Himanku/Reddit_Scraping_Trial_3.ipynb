{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Reddit Scraping Trial 3.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "zn2iBPb3Arbn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "outputId": "4edbcc82-3a74-4f24-cd5c-f9a298134767"
      },
      "source": [
        "pip install praw"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting praw\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/2c/15/4bcc44271afce0316c73cd2ed35f951f1363a07d4d5d5440ae5eb2baad78/praw-7.1.0-py3-none-any.whl (152kB)\n",
            "\r\u001b[K     |██▏                             | 10kB 16.5MB/s eta 0:00:01\r\u001b[K     |████▎                           | 20kB 2.1MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 30kB 2.7MB/s eta 0:00:01\r\u001b[K     |████████▋                       | 40kB 3.1MB/s eta 0:00:01\r\u001b[K     |██████████▊                     | 51kB 2.5MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 61kB 2.7MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 71kB 3.0MB/s eta 0:00:01\r\u001b[K     |█████████████████▎              | 81kB 3.2MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 92kB 3.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████▌          | 102kB 3.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 112kB 3.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▉      | 122kB 3.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 133kB 3.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▏ | 143kB 3.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 153kB 3.4MB/s \n",
            "\u001b[?25hCollecting update-checker>=0.17\n",
            "  Downloading https://files.pythonhosted.org/packages/d6/c3/aaf8a162df8e8f9d321237c7c0e63aff95b42d19f1758f96606e3cabb245/update_checker-0.17-py2.py3-none-any.whl\n",
            "Collecting prawcore<2.0,>=1.3.0\n",
            "  Downloading https://files.pythonhosted.org/packages/61/89/651078078f089c182164efd10d45d3b709b46e4e40b47e63759e76349dc1/prawcore-1.4.0-py3-none-any.whl\n",
            "Collecting websocket-client>=0.54.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4c/5f/f61b420143ed1c8dc69f9eaec5ff1ac36109d52c80de49d66e0c36c3dfdf/websocket_client-0.57.0-py2.py3-none-any.whl (200kB)\n",
            "\u001b[K     |████████████████████████████████| 204kB 10.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from update-checker>=0.17->praw) (2.23.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from websocket-client>=0.54.0->praw) (1.15.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.3.0->update-checker>=0.17->praw) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.3.0->update-checker>=0.17->praw) (2020.6.20)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.3.0->update-checker>=0.17->praw) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.3.0->update-checker>=0.17->praw) (3.0.4)\n",
            "Installing collected packages: update-checker, prawcore, websocket-client, praw\n",
            "Successfully installed praw-7.1.0 prawcore-1.4.0 update-checker-0.17 websocket-client-0.57.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FpLMJaQEAu81",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 392
        },
        "outputId": "70212c98-d238-4731-e2de-a9d139ba3927"
      },
      "source": [
        "import pandas as pd\n",
        "import praw\n",
        "import json\n",
        "import time\n",
        "import numpy as np\n",
        "from multiprocessing import Process\n",
        "import sys\n",
        "import os\n",
        "import datetime\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pymongo\n",
        "\n",
        "DBNAME = \"reddit_capstone425\"\n",
        "\n",
        "def connect_to_mongo(dbname = DBNAME):\n",
        "    with open('keys/mongoconnect.txt') as f:\n",
        "        s = f.read()\n",
        "    s = s[:-1]\n",
        "    client = pymongo.MongoClient(s)\n",
        "    db = client.get_database(dbname)\n",
        "    return db\n",
        "\n",
        "def do_list_of_subs(subreddit_list,keys,date,user_list):\n",
        "    '''\n",
        "    This is the task for each process to complete when multiprocessing data scraping\n",
        "    from reddit api\n",
        "    '''\n",
        "    #start reddit instance\n",
        "    reddit = praw.Reddit(client_secret = keys[1],\n",
        "                         client_id = keys[0] ,\n",
        "                         user_agent = 'datagathering by /u/GougeC')\n",
        "    #start connection to mongodb database\n",
        "    #loop over each sub assigned to this process\n",
        "    for sub_name in subreddit_list:\n",
        "        get_write_sub_data(sub_name,date,reddit,user_list)\n",
        "\n",
        "def get_subreddits():\n",
        "    '''\n",
        "    Scrapes redditlist.com for the top 1250 subreddits and then\n",
        "    drops some of the more offensive/inappropriate subs from the list\n",
        "    before returning a list of the subs\n",
        "    '''\n",
        "    subs = []\n",
        "    for page in range(1,11):\n",
        "        req = requests.get('http://redditlist.com/?page={}'.format(page))\n",
        "        soup = BeautifulSoup(req.text,'lxml')\n",
        "        top = soup.find_all(class_='span4 listing')[1]\n",
        "        soup2 = top.find_all(class_='subreddit-url')\n",
        "        for x in soup2:\n",
        "            soup3 = x.find_all(class_='sfw')[0].text\n",
        "            subs.append(soup3)\n",
        "    #subs that I deemed \"undesirable\" for my recommender (innapropriate or hostile communities)\n",
        "    drop_list = ['LadyBoners','Celebs', 'pussypassdenied','MensRights','jesuschristreddit','TheRedPill','NoFap']\n",
        "\n",
        "    for x in subs:\n",
        "        if x in drop_list:\n",
        "            subs.remove(x)\n",
        "    return subs\n",
        "\n",
        "def get_post_info(post,user_list,subreddit):\n",
        "    '''\n",
        "    Input: a PRAW post object\n",
        "    Output: a post dictionary with data about the post imputed\n",
        "    including all the top comments and children (up to 1000 comments)\n",
        "    '''\n",
        "    post_dict = {}\n",
        "    post_dict['title'] = post.title\n",
        "    post_dict['id'] = post.id\n",
        "    post_dict['permalink'] = post.permalink\n",
        "    post_dict['subreddit'] = subreddit\n",
        "    if post.author:\n",
        "        if post.author.name:\n",
        "            post_dict['author'] = post.author.name\n",
        "            with open(user_list,'a') as f:\n",
        "                f.write(post.author.name)\n",
        "                f.write(',\\n')\n",
        "                f.flush()\n",
        "\n",
        "    post_dict['selftext'] = post.selftext\n",
        "    post_dict['domain'] = post.domain\n",
        "    post_dict['link_url'] = post.url\n",
        "    comment_list = []\n",
        "    if post.comments:\n",
        "        post.comments.replace_more()\n",
        "        for comment in post.comments:\n",
        "            if comment.body:\n",
        "                comment_list.append(comment.body)\n",
        "            if len(comment_list) > 1000:\n",
        "                break\n",
        "            #if comment.author:\n",
        "            #    with open(user_list,'a+') as g:\n",
        "            #        g.write(comment.author.name+',\\n')\n",
        "            #        g.flush()\n",
        "            #try:\n",
        "            #    if comment.replies:\n",
        "            #        reps = get_10_children(comment,user_list)\n",
        "            #        comment_list+=reps\n",
        "            #except Exception as e:\n",
        "            #    print('trying to get children broke',str(e))\n",
        "            #if len(comment_list) >= 1000: break\n",
        "    post_dict['comments'] = comment_list\n",
        "    return post_dict\n",
        "\n",
        "def get_10_children(comment,user_list):\n",
        "    '''\n",
        "    given a reddit comment object in PRAW this returns the text and users\n",
        "    from 10 children comments\n",
        "    '''\n",
        "    comments = []\n",
        "    i = 0\n",
        "    #check if the replies exist\n",
        "    if comment.replies:\n",
        "        comment.replies.replace_more()\n",
        "        i = 0\n",
        "        #get data from up to ten replies\n",
        "        for reply in comment.replies:\n",
        "            i+=1\n",
        "            if i==10: break\n",
        "            if reply.body:\n",
        "                comments.append(reply.body)\n",
        "    return comments\n",
        "\n",
        "\n",
        "def get_write_sub_data(sub_name,date,reddit,user_list):\n",
        "    '''\n",
        "    Retrives the data from the sub named in the sub_name parameter\n",
        "    and writes the data as a json with the date included in the filename\n",
        "    '''\n",
        "    t1 = time.time()\n",
        "\n",
        "    print(\"trying to get \",sub_name)\n",
        "    #call reddit api to get the sub data\n",
        "    try:\n",
        "        sub = reddit.subreddit(sub_name)\n",
        "    except:\n",
        "        print('trying to get subbreddit broke',sub_name)\n",
        "        with open('../data'+date+'/'+'failed_subs'+date+'.txt','a') as f:\n",
        "            f.write(sub_name+', ')\n",
        "        return None\n",
        "\n",
        "    #loop over all the posts in the subreddit object up to 100 posts\n",
        "    posts = {}\n",
        "    top = sub.top(time_filter = 'month')\n",
        "    i = 0\n",
        "    db = connect_to_mongo()\n",
        "    posts = db.posts\n",
        "    try:\n",
        "\n",
        "        for post in top:\n",
        "            post_dic = {}\n",
        "            i+=1\n",
        "            #get metadata from post plus post data from deeper function\n",
        "            post_dic['subreddit'] = sub_name\n",
        "            post_dic['permalink'] = post.permalink\n",
        "            post_dic['data'] = get_post_info(post,user_list,sub_name)\n",
        "            #write post data to mongodb\n",
        "            posts.insert_one(post_dic)\n",
        "\n",
        "    except Exception as e:\n",
        "        print('trying to loop through posts broke',sub_name,e)\n",
        "        return None\n",
        "            #vestigial code for now\n",
        "            ##filename = '../data'+date+'/'+sub_name+date+'.json'\n",
        "            ##print('writing ',sub_name,\" as \", filename)\n",
        "            ##subreddit_data = {'subreddit':sub_name,'posts':posts}\n",
        "            ##db = client.capstone_db\n",
        "            ##subs = db.subreddits\n",
        "            ##subs.insert(subreddit_data)\n",
        "    t2 = time.time()\n",
        "    elapsed = t2 - t1\n",
        "    print(\"finished sub: {}. It took {} seconds.\".format(sub_name,elapsed))\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    # get subreddits from scraping function\n",
        "    sublist = get_subreddits()\n",
        "\n",
        "    #divide list of subs into 4 parts\n",
        "    n = len(sublist)//8\n",
        "    print('attempting to get',len(sublist), 'subreddits')\n",
        "    lists = []\n",
        "    for i in range(0,9):\n",
        "        lists.append(sublist[i*n:(i+1)*n])\n",
        "    processes = []\n",
        "    n = datetime.datetime.now()\n",
        "    date = \"_\"+str(n.month)+\"_\"+str(n.day)\n",
        "    directory = '../data'+date+'/'\n",
        "\n",
        "    #make directories\n",
        "    if not os.path.exists(directory):\n",
        "        os.makedirs(directory)\n",
        "    open(directory+'failed_subs'+date+'.txt','w+').close()\n",
        "\n",
        "    #create each process and assign it its work\n",
        "    for i in range(1,9):\n",
        "        keys = np.loadtxt('keys/reddit{}.txt'.format(i),dtype=str,delimiter=',')\n",
        "        user_list = directory+'users_list'+date+str(i)+'.txt'\n",
        "        open(user_list,'w+').close()\n",
        "        p = Process(target=do_list_of_subs, args = (lists[i-1],keys,date,user_list))\n",
        "        processes.append(p)\n",
        "\n",
        "    #start and close each process\n",
        "    for p in processes:\n",
        "        p.start()\n",
        "    for p in processes:\n",
        "        p.join()\n",
        "    print('Processes Finished for subreddit data')\n",
        "\n",
        "    # #read user data that was deposited earlier by each process\n",
        "    # users1 = pd.read_csv(directory+'users_list'+date+str(1)+'.txt',header=None)[0]\n",
        "    # users2 = pd.read_csv(directory+'users_list'+date+str(2)+'.txt',header=None)[0]\n",
        "    # users3 = pd.read_csv(directory+'users_list'+date+str(3)+'.txt',header=None)[0]\n",
        "    # users4 = pd.read_csv(directory+'users_list'+date+str(4)+'.txt',header=None)[0]\n",
        "    # bagousers = set()\n",
        "    #\n",
        "    # #get a set of the unique users\n",
        "    # for lst in [users1,users2,users3,users4]:\n",
        "    #     uniq = lst.unique()\n",
        "    #     for user in uniq:\n",
        "    #         bagousers.add(user)\n",
        "    # users_unique = list(bagousers)\n",
        "    #\n",
        "    # #split the user set into 4, for each process\n",
        "    # k = len(users_unique)//4\n",
        "    # lists = [users_unique[:k],users_unique[k:2*k],users_unique[2*k,3*k],users_unique[3*k:]]\n",
        "    #\n",
        "    # #assign each process its work and its api keys\n",
        "    # for i in range(1,5):\n",
        "    #     keys = np.loadtxt('keys/reddit{}.txt'.format(i),dtype=str,delimiter=',')\n",
        "    #     filename = '../data'+date+'/'+'USER_DATA_'+str(i)+'.json'\n",
        "    #     p = Process(target=grud.get_data_for_userlist, args = (lists[i-1],keys,filename,client))\n",
        "    #     processes.append(p)\n",
        "    #\n",
        "    # #start and finish each process\n",
        "    # for p in processes:\n",
        "    #     p.start()\n",
        "    # for p in processes:\n",
        "    #     p.join()\n",
        "    # print('Processes Finished for User Data')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "attempting to get 1245 subreddits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "OSError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-018f2fd8866f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    194\u001b[0m     \u001b[0;31m#create each process and assign it its work\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m9\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m         \u001b[0mkeys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloadtxt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'keys/reddit{}.txt'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdelimiter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m','\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m         \u001b[0muser_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdirectory\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'users_list'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mdate\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'.txt'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m         \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muser_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'w+'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36mloadtxt\u001b[0;34m(fname, dtype, comments, delimiter, converters, skiprows, usecols, unpack, ndmin, encoding, max_rows)\u001b[0m\n\u001b[1;32m    979\u001b[0m             \u001b[0mfname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos_fspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    980\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_is_string_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 981\u001b[0;31m             \u001b[0mfh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_datasource\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    982\u001b[0m             \u001b[0mfencoding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'encoding'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'latin1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    983\u001b[0m             \u001b[0mfh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/numpy/lib/_datasource.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(path, mode, destpath, encoding, newline)\u001b[0m\n\u001b[1;32m    267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m     \u001b[0mds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataSource\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdestpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 269\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnewline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnewline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    270\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/numpy/lib/_datasource.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(self, path, mode, encoding, newline)\u001b[0m\n\u001b[1;32m    621\u001b[0m                                       encoding=encoding, newline=newline)\n\u001b[1;32m    622\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 623\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%s not found.\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    624\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOSError\u001b[0m: keys/reddit1.txt not found."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dP9Mjpe_A2Q5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}